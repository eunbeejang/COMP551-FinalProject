{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import h5py\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "IMDB_train = pd.read_csv('./IMDB-train.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_train_y = IMDB_train[:][1]\n",
    "IMDB_valid = pd.read_csv('./IMDB-valid.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_valid_y = IMDB_valid[:][1]\n",
    "IMDB_test = pd.read_csv('./IMDB-test.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_test_y = IMDB_test[:][1]\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [IMDB_train, IMDB_valid]\n",
    "frames_y = [IMDB_train_y, IMDB_valid_y]\n",
    "IMDB_train = pd.concat(frames)\n",
    "IMDB_train_y = pd.concat(frames_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train = IMDB_train[:][0]\n",
    "#IMDB_test = IMDB_test[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       For a movie that gets no respect there sure ar...\n",
       "1       Bizarre horror movie filled with famous faces ...\n",
       "2       A solid, if unremarkable film. Matthau, as Ein...\n",
       "3       It's a strange feeling to sit alone in a theat...\n",
       "4       You probably all already know this by now, but...\n",
       "5       I saw the movie with two grown children. Altho...\n",
       "6       You're using the IMDb.<br /><br />You've given...\n",
       "7       This was a good film with a powerful message o...\n",
       "8       Made after QUARTET was, TRIO continued the qua...\n",
       "9       For a mature man, to admit that he shed a tear...\n",
       "10      Aileen Gonsalves, my girlfriend, is in this fi...\n",
       "11      Jonathan Demme's directorial debut for Roger C...\n",
       "12      When I rented this movie to watch it, I knew t...\n",
       "13      It's hard to say sometimes why exactly a film ...\n",
       "14      Yes, this gets the full ten stars. It's plain ...\n",
       "15      Hello. This movie is.......well.......okay. Ju...\n",
       "16      This is a film that was very well done. I had ...\n",
       "17      A typical romp through Cheech and Chong's real...\n",
       "18      OK heres what I say: <br /><br />The movie was...\n",
       "19      This is one of the first films I can remember,...\n",
       "20      This film is worth seeing alone for Jared Harr...\n",
       "21      I was 10 years old when this show was on TV. B...\n",
       "22      Here's another movie that should be loaded int...\n",
       "23      To all the reviewers on this page, I would hav...\n",
       "24      A favourite of mine,this movie tells of two fe...\n",
       "25      As Most Off You Might off Seen Star Wars: Retu...\n",
       "26      I thought this movie was LOL funny. It's a fun...\n",
       "27      Hood of the Living Dead had a lot to live up t...\n",
       "28      Pierce Brosnan the newest but no longer James ...\n",
       "29      This is one of those Film's/pilot that if you ...\n",
       "                              ...                        \n",
       "9970    Shame on Yash Raj films and Aditya Chopra who ...\n",
       "9971    OK, look at the title of this film.<br /><br /...\n",
       "9972    Start of with the good bit: several times Sway...\n",
       "9973    After seeing this film I complained to my loca...\n",
       "9974    Really bad movie, the story is too simple and ...\n",
       "9975    Although there's Flying Guillotines as part of...\n",
       "9976    We all know what's like when we have a bad day...\n",
       "9977    Director Warren Beatty's intention to turn Che...\n",
       "9978    What made the idea of seeing this movie so att...\n",
       "9979    ...for one of the worst Swedish movies ever......\n",
       "9980    This \"movie\" is more like a music video. Kustu...\n",
       "9981    I thought it was not the best re-cap episode I...\n",
       "9982    Scary.. Yes Scary!! Jam-packed with nudity (fr...\n",
       "9983    I am a big fan of the Spaghetti Western Genre,...\n",
       "9984    This Drummond entry is lacking in continuity. ...\n",
       "9985    ...for this movie defines a new low in Bollywo...\n",
       "9986    This sad little film bears little similarity t...\n",
       "9987    Hollywood Hotel was the last movie musical tha...\n",
       "9988    This film is a flagrant rip-off of one of the ...\n",
       "9989    Prior to watching \"Dahmer,\" I thought no movie...\n",
       "9990    Leave it to geniuses like Ventura Pons, the Sp...\n",
       "9991    The script is so so laughable... this in turn,...\n",
       "9992    It was nice to see all the familiar characters...\n",
       "9993    As far as Spaghetti Westerns go, I'd put This ...\n",
       "9994    Movie had some good acting and good moments (t...\n",
       "9995    The Wayward Cloud is a frustrating film to wat...\n",
       "9996    For your own good, it would be best to disrega...\n",
       "9997    In my life I have seen many great and awful mo...\n",
       "9998    OK. I know that the wanna-be John Hughes movie...\n",
       "9999    Whoever made this movie must have done it as a...\n",
       "Name: 0, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    new_data = []\n",
    "    #i = 0\n",
    "    for sentence in (data[:][0]):\n",
    "        #clean = re.compile('<.*?>')\n",
    "        new_sentence = re.sub('<.*?>', '', sentence) # remove HTML tags\n",
    "        new_sentence = re.sub(r'[^\\w\\s]', '', new_sentence) # remove punctuation\n",
    "        new_sentence = new_sentence.lower() # convert to lower case\n",
    "        if new_sentence != '':\n",
    "            new_data.append(new_sentence)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I saw the movie with two grown children. Altho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You're using the IMDb.&lt;br /&gt;&lt;br /&gt;You've given...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This was a good film with a powerful message o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Made after QUARTET was, TRIO continued the qua...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>For a mature man, to admit that he shed a tear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aileen Gonsalves, my girlfriend, is in this fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Jonathan Demme's directorial debut for Roger C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>When I rented this movie to watch it, I knew t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>It's hard to say sometimes why exactly a film ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Yes, this gets the full ten stars. It's plain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hello. This movie is.......well.......okay. Ju...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>This is a film that was very well done. I had ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A typical romp through Cheech and Chong's real...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OK heres what I say: &lt;br /&gt;&lt;br /&gt;The movie was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>This is one of the first films I can remember,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>This film is worth seeing alone for Jared Harr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I was 10 years old when this show was on TV. B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Here's another movie that should be loaded int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>To all the reviewers on this page, I would hav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A favourite of mine,this movie tells of two fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>As Most Off You Might off Seen Star Wars: Retu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I thought this movie was LOL funny. It's a fun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hood of the Living Dead had a lot to live up t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Pierce Brosnan the newest but no longer James ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>This is one of those Film's/pilot that if you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>Shame on Yash Raj films and Aditya Chopra who ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>OK, look at the title of this film.&lt;br /&gt;&lt;br /...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>Start of with the good bit: several times Sway...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>After seeing this film I complained to my loca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>Really bad movie, the story is too simple and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>Although there's Flying Guillotines as part of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>We all know what's like when we have a bad day...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>Director Warren Beatty's intention to turn Che...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>What made the idea of seeing this movie so att...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>...for one of the worst Swedish movies ever......</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>This \"movie\" is more like a music video. Kustu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>I thought it was not the best re-cap episode I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>Scary.. Yes Scary!! Jam-packed with nudity (fr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>I am a big fan of the Spaghetti Western Genre,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>This Drummond entry is lacking in continuity. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>...for this movie defines a new low in Bollywo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>This sad little film bears little similarity t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>Hollywood Hotel was the last movie musical tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>This film is a flagrant rip-off of one of the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>Prior to watching \"Dahmer,\" I thought no movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>Leave it to geniuses like Ventura Pons, the Sp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>The script is so so laughable... this in turn,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>It was nice to see all the familiar characters...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>As far as Spaghetti Westerns go, I'd put This ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Movie had some good acting and good moments (t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>The Wayward Cloud is a frustrating film to wat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>For your own good, it would be best to disrega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>In my life I have seen many great and awful mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>OK. I know that the wanna-be John Hughes movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Whoever made this movie must have done it as a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  1\n",
       "0     For a movie that gets no respect there sure ar...  1\n",
       "1     Bizarre horror movie filled with famous faces ...  1\n",
       "2     A solid, if unremarkable film. Matthau, as Ein...  1\n",
       "3     It's a strange feeling to sit alone in a theat...  1\n",
       "4     You probably all already know this by now, but...  1\n",
       "5     I saw the movie with two grown children. Altho...  1\n",
       "6     You're using the IMDb.<br /><br />You've given...  1\n",
       "7     This was a good film with a powerful message o...  1\n",
       "8     Made after QUARTET was, TRIO continued the qua...  1\n",
       "9     For a mature man, to admit that he shed a tear...  1\n",
       "10    Aileen Gonsalves, my girlfriend, is in this fi...  1\n",
       "11    Jonathan Demme's directorial debut for Roger C...  1\n",
       "12    When I rented this movie to watch it, I knew t...  1\n",
       "13    It's hard to say sometimes why exactly a film ...  1\n",
       "14    Yes, this gets the full ten stars. It's plain ...  1\n",
       "15    Hello. This movie is.......well.......okay. Ju...  1\n",
       "16    This is a film that was very well done. I had ...  1\n",
       "17    A typical romp through Cheech and Chong's real...  1\n",
       "18    OK heres what I say: <br /><br />The movie was...  1\n",
       "19    This is one of the first films I can remember,...  1\n",
       "20    This film is worth seeing alone for Jared Harr...  1\n",
       "21    I was 10 years old when this show was on TV. B...  1\n",
       "22    Here's another movie that should be loaded int...  1\n",
       "23    To all the reviewers on this page, I would hav...  1\n",
       "24    A favourite of mine,this movie tells of two fe...  1\n",
       "25    As Most Off You Might off Seen Star Wars: Retu...  1\n",
       "26    I thought this movie was LOL funny. It's a fun...  1\n",
       "27    Hood of the Living Dead had a lot to live up t...  1\n",
       "28    Pierce Brosnan the newest but no longer James ...  1\n",
       "29    This is one of those Film's/pilot that if you ...  1\n",
       "...                                                 ... ..\n",
       "9970  Shame on Yash Raj films and Aditya Chopra who ...  0\n",
       "9971  OK, look at the title of this film.<br /><br /...  0\n",
       "9972  Start of with the good bit: several times Sway...  0\n",
       "9973  After seeing this film I complained to my loca...  0\n",
       "9974  Really bad movie, the story is too simple and ...  0\n",
       "9975  Although there's Flying Guillotines as part of...  0\n",
       "9976  We all know what's like when we have a bad day...  0\n",
       "9977  Director Warren Beatty's intention to turn Che...  0\n",
       "9978  What made the idea of seeing this movie so att...  0\n",
       "9979  ...for one of the worst Swedish movies ever......  0\n",
       "9980  This \"movie\" is more like a music video. Kustu...  0\n",
       "9981  I thought it was not the best re-cap episode I...  0\n",
       "9982  Scary.. Yes Scary!! Jam-packed with nudity (fr...  0\n",
       "9983  I am a big fan of the Spaghetti Western Genre,...  0\n",
       "9984  This Drummond entry is lacking in continuity. ...  0\n",
       "9985  ...for this movie defines a new low in Bollywo...  0\n",
       "9986  This sad little film bears little similarity t...  0\n",
       "9987  Hollywood Hotel was the last movie musical tha...  0\n",
       "9988  This film is a flagrant rip-off of one of the ...  0\n",
       "9989  Prior to watching \"Dahmer,\" I thought no movie...  0\n",
       "9990  Leave it to geniuses like Ventura Pons, the Sp...  0\n",
       "9991  The script is so so laughable... this in turn,...  0\n",
       "9992  It was nice to see all the familiar characters...  0\n",
       "9993  As far as Spaghetti Westerns go, I'd put This ...  0\n",
       "9994  Movie had some good acting and good moments (t...  0\n",
       "9995  The Wayward Cloud is a frustrating film to wat...  0\n",
       "9996  For your own good, it would be best to disrega...  0\n",
       "9997  In my life I have seen many great and awful mo...  0\n",
       "9998  OK. I know that the wanna-be John Hughes movie...  0\n",
       "9999  Whoever made this movie must have done it as a...  0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef rm_stopwords(data):\\n    new_data = []\\n    for sent in data:\\n        new_data.append([w for w in sent if w not in stopwords])\\n    return new_data\\n'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def rm_stopwords(data):\n",
    "    new_data = []\n",
    "    for sent in data:\n",
    "        new_data.append([w for w in sent if w not in stopwords])\n",
    "    return new_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<<<<IGNORE THESE FOR NOW>>>>\\n\\ndef tokenize(data):\\n    new_data = []\\n    for sentence in (data):\\n        new_sentence = nltk.word_tokenize(sentence)\\n        new_data.append(new_sentence)\\n    return new_data        \\n\\ndef stem_lem(data):\\n    new_data = []\\n    for sent in data:\\n        this_sent = []\\n        for w in test3:\\n            w = stemmer.stem(w)\\n            w = lemmatizer.lemmatize(w)\\n            this_sent.append(stemmer.stem(w))\\n        new_data.append(this_sent)\\n    return new_data\\n'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "<<<<IGNORE THESE FOR NOW>>>>\n",
    "\n",
    "def tokenize(data):\n",
    "    new_data = []\n",
    "    for sentence in (data):\n",
    "        new_sentence = nltk.word_tokenize(sentence)\n",
    "        new_data.append(new_sentence)\n",
    "    return new_data        \n",
    "\n",
    "def stem_lem(data):\n",
    "    new_data = []\n",
    "    for sent in data:\n",
    "        this_sent = []\n",
    "        for w in test3:\n",
    "            w = stemmer.stem(w)\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            this_sent.append(stemmer.stem(w))\n",
    "        new_data.append(this_sent)\n",
    "    return new_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMDB_train = preprocessing(IMDB_train)\n",
    "#IMDB_valid = preprocessing(IMDB_valid)\n",
    "IMDB_test = preprocessing(IMDB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train_tok = tokenize(IMDB_train)\n",
    "#IMDB_valid_tok = tokenize(IMDB_valid)\n",
    "#IMDB_test_tok = tokenize(IMDB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train_sl = stem_lem(IMDB_train_tok)\n",
    "#IMDB_valid_sl = stem_lem(IMDB_valid_tok)\n",
    "#IMDB_test_sl = stem_lem(IMDB_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train_stop = rm_stopwords(IMDB_train_sl)\n",
    "#IMDB_valid_stop = rm_stopwords(IMDB_valid_sl)\n",
    "#IMDB_test_stop = rm_stopwords(IMDB_test_sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of n-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "unigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 1), stop_words='english', max_features =30000)\n",
    "bigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(2, 2), stop_words='english', max_features =30000)\n",
    "trigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(3, 3), stop_words='english', max_features =30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unigram = unigram.fit_transform(IMDB_train).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rm_sents(data, target):\n",
    "    new_data = []\n",
    "    new_target = []\n",
    "    for i in range(0,len(data)):\n",
    "        if len(list(set(data[i]))) is not 1:\n",
    "            new_data.append(data[i])\n",
    "            new_target.append(target[i])\n",
    "    return new_data, new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data, n_y = rm_sents(train_unigram, np.asarray(IMDB_train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unigram = unigram.fit_transform(IMDB_train).toarray()\n",
    "test_unigram = unigram.transform(IMDB_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 30000)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unigram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-ea81a177ee1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDB_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDB_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    812\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "train_bigram = bigram.fit_transform(IMDB_train).toarray()\n",
    "test_bigram = bigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_trigram = trigram.fit_transform(IMDB_train).toarray()\n",
    "test_trigram = trigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(test_bigram)\n",
    "np.save('./train_unigram', train_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('train_unigram.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"default\", data=train_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(train_unigram).to_csv('./train_unigram.csv', index = False, header = False)\n",
    "#pd.DataFrame.from_records(train_bigram).to_csv('./train_bigram.csv', index = False, header = False)\n",
    "#pd.DataFrame.from_records(train_trigram).to_csv('./train_trigram.csv', index = False, header = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "unigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 1), stop_words=None, max_features =30000)\n",
    "bigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(2, 2), stop_words=None, max_features =30000)\n",
    "trigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(3, 3), stop_words=None, max_features =30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unigram_w_sw = unigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_unigram_w_sw = unigram_w_sw.transform(IMDB_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bigram_w_sw = bigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_bigram_w_sw = bigram_w_sw.transform(IMDB_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_trigram_w_sw = trigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_trigram_w_sw = trigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(train_unigram_w_sw).to_csv('./train_unigram_w_sw.csv', index = False, header = False)\n",
    "pd.DataFrame.from_records(train_bigram_w_sw).to_csv('./train_bigram_w_sw.csv', index = False, header = False)\n",
    "pd.DataFrame.from_records(train_trigram_w_sw).to_csv('./train_trigram_w_sw.csv', index = False, header = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
