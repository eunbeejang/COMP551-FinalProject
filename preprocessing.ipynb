{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import h5py\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "IMDB_train = pd.read_csv('./IMDB-train.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_train_y = IMDB_train[:][1]\n",
    "IMDB_valid = pd.read_csv('./IMDB-valid.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_valid_y = IMDB_valid[:][1]\n",
    "IMDB_test = pd.read_csv('./IMDB-test.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_test_y = IMDB_test[:][1]\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [IMDB_train, IMDB_valid]\n",
    "frames_y = [IMDB_train_y, IMDB_valid_y]\n",
    "IMDB_train = pd.concat(frames)\n",
    "IMDB_train_y = pd.concat(frames_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    new_data = []\n",
    "    #i = 0\n",
    "    for sentence in (data[:][0]):\n",
    "        #clean = re.compile('<.*?>')\n",
    "        new_sentence = re.sub('<.*?>', '', sentence) # remove HTML tags\n",
    "        new_sentence = re.sub(r'[^\\w\\s]', '', new_sentence) # remove punctuation\n",
    "        new_sentence = new_sentence.lower() # convert to lower case\n",
    "        if new_sentence != '':\n",
    "            new_data.append(new_sentence)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef rm_stopwords(data):\\n    new_data = []\\n    for sent in data:\\n        new_data.append([w for w in sent if w not in stopwords])\\n    return new_data\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def rm_stopwords(data):\n",
    "    new_data = []\n",
    "    for sent in data:\n",
    "        new_data.append([w for w in sent if w not in stopwords])\n",
    "    return new_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<<<<IGNORE THESE FOR NOW>>>>\\n\\ndef tokenize(data):\\n    new_data = []\\n    for sentence in (data):\\n        new_sentence = nltk.word_tokenize(sentence)\\n        new_data.append(new_sentence)\\n    return new_data        \\n\\ndef stem_lem(data):\\n    new_data = []\\n    for sent in data:\\n        this_sent = []\\n        for w in test3:\\n            w = stemmer.stem(w)\\n            w = lemmatizer.lemmatize(w)\\n            this_sent.append(stemmer.stem(w))\\n        new_data.append(this_sent)\\n    return new_data\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "<<<<IGNORE THESE FOR NOW>>>>\n",
    "\n",
    "def tokenize(data):\n",
    "    new_data = []\n",
    "    for sentence in (data):\n",
    "        new_sentence = nltk.word_tokenize(sentence)\n",
    "        new_data.append(new_sentence)\n",
    "    return new_data        \n",
    "\n",
    "def stem_lem(data):\n",
    "    new_data = []\n",
    "    for sent in data:\n",
    "        this_sent = []\n",
    "        for w in test3:\n",
    "            w = stemmer.stem(w)\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            this_sent.append(stemmer.stem(w))\n",
    "        new_data.append(this_sent)\n",
    "    return new_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_train = preprocessing(IMDB_train)\n",
    "#IMDB_valid = preprocessing(IMDB_valid)\n",
    "IMDB_test = preprocessing(IMDB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train_tok = tokenize(IMDB_train)\n",
    "#IMDB_valid_tok = tokenize(IMDB_valid)\n",
    "#IMDB_test_tok = tokenize(IMDB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train_sl = stem_lem(IMDB_train_tok)\n",
    "#IMDB_valid_sl = stem_lem(IMDB_valid_tok)\n",
    "#IMDB_test_sl = stem_lem(IMDB_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train_stop = rm_stopwords(IMDB_train_sl)\n",
    "#IMDB_valid_stop = rm_stopwords(IMDB_valid_sl)\n",
    "#IMDB_test_stop = rm_stopwords(IMDB_test_sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of n-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 1), stop_words='english', max_features =30000)\n",
    "bigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(2, 2), stop_words='english', max_features =30000)\n",
    "trigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(3, 3), stop_words='english', max_features =30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigram = unigram.fit_transform(IMDB_train).toarray()\n",
    "test_unigram = unigram.transform(IMDB_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bigram = bigram.fit_transform(IMDB_train).toarray()\n",
    "test_bigram = bigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_trigram = trigram.fit_transform(IMDB_train).toarray()\n",
    "test_trigram = trigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_bigram)\n",
    "np.save('./train_unigram', train_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('train_unigram.hdf5', 'w') as f:\n",
    "    dset = f.create_dataset(\"default\", data=train_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(train_unigram).to_csv('./train_unigram.csv', index = False, header = False)\n",
    "#pd.DataFrame.from_records(train_bigram).to_csv('./train_bigram.csv', index = False, header = False)\n",
    "#pd.DataFrame.from_records(train_trigram).to_csv('./train_trigram.csv', index = False, header = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "unigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 1), stop_words=None, max_features =30000)\n",
    "bigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(2, 2), stop_words=None, max_features =30000)\n",
    "trigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(3, 3), stop_words=None, max_features =30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unigram_w_sw = unigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_unigram_w_sw = unigram_w_sw.transform(IMDB_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigram_w_sw = bigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_bigram_w_sw = bigram_w_sw.transform(IMDB_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_trigram_w_sw = trigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_trigram_w_sw = trigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npd.DataFrame.from_records(train_trigram).to_csv('./train_trigram.csv', index = False, header = False)\\npd.DataFrame.from_records(valid_trigram).to_csv('./valid_trigram.csv', index = False, header = False)\\npd.DataFrame.from_records(test_trigram).to_csv('./test_trigram.csv', index = False, header = False)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(train_unigram_w_sw).to_csv('./train_unigram_w_sw.csv', index = False, header = False)\n",
    "pd.DataFrame.from_records(train_bigram_w_sw).to_csv('./train_bigram_w_sw.csv', index = False, header = False)\n",
    "pd.DataFrame.from_records(train_trigram_w_sw).to_csv('./train_trigram_w_sw.csv', index = False, header = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_linearsvm_clf = LinearSVC(max_iter=15000)\n",
    "list_C = [2*i for i in range(1, 5)]\n",
    "list_tol = [10**(-i*2) for i in range(1, 4)]\n",
    "# parameter grid to check against\n",
    "# for hyperparameter tuning\n",
    "tuned_parameters = [{'C': list_C, 'tol': list_tol}]\n",
    "imdb_linearsvm_clf = GridSearchCV(imdb_linearsvm_clf, tuned_parameters, scoring='f1_micro', cv=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] C=2, tol=0.01 ...................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... C=2, tol=0.01, total=  22.0s\n",
      "[CV] C=2, tol=0.01 ...................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... C=2, tol=0.01, total=  19.6s\n",
      "[CV] C=2, tol=0.01 ...................................................\n",
      "[CV] .................................... C=2, tol=0.01, total=  19.4s\n",
      "[CV] C=2, tol=0.0001 .................................................\n",
      "[CV] .................................. C=2, tol=0.0001, total=  23.0s\n",
      "[CV] C=2, tol=0.0001 .................................................\n",
      "[CV] .................................. C=2, tol=0.0001, total=  21.5s\n",
      "[CV] C=2, tol=0.0001 .................................................\n",
      "[CV] .................................. C=2, tol=0.0001, total=  21.4s\n",
      "[CV] C=2, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=2, tol=1e-06, total=  34.1s\n",
      "[CV] C=2, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=2, tol=1e-06, total=  28.5s\n",
      "[CV] C=2, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=2, tol=1e-06, total=  28.3s\n",
      "[CV] C=4, tol=0.01 ...................................................\n",
      "[CV] .................................... C=4, tol=0.01, total=  18.2s\n",
      "[CV] C=4, tol=0.01 ...................................................\n",
      "[CV] .................................... C=4, tol=0.01, total=  17.0s\n",
      "[CV] C=4, tol=0.01 ...................................................\n",
      "[CV] .................................... C=4, tol=0.01, total=  17.1s\n",
      "[CV] C=4, tol=0.0001 .................................................\n",
      "[CV] .................................. C=4, tol=0.0001, total=  24.0s\n",
      "[CV] C=4, tol=0.0001 .................................................\n",
      "[CV] .................................. C=4, tol=0.0001, total=  20.1s\n",
      "[CV] C=4, tol=0.0001 .................................................\n",
      "[CV] .................................. C=4, tol=0.0001, total=  23.9s\n",
      "[CV] C=4, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=4, tol=1e-06, total=  41.6s\n",
      "[CV] C=4, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=4, tol=1e-06, total=  38.0s\n",
      "[CV] C=4, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=4, tol=1e-06, total=  34.6s\n",
      "[CV] C=6, tol=0.01 ...................................................\n",
      "[CV] .................................... C=6, tol=0.01, total=  16.9s\n",
      "[CV] C=6, tol=0.01 ...................................................\n",
      "[CV] .................................... C=6, tol=0.01, total=  16.7s\n",
      "[CV] C=6, tol=0.01 ...................................................\n",
      "[CV] .................................... C=6, tol=0.01, total=  16.8s\n",
      "[CV] C=6, tol=0.0001 .................................................\n",
      "[CV] .................................. C=6, tol=0.0001, total=  24.7s\n",
      "[CV] C=6, tol=0.0001 .................................................\n",
      "[CV] .................................. C=6, tol=0.0001, total=  19.3s\n",
      "[CV] C=6, tol=0.0001 .................................................\n",
      "[CV] .................................. C=6, tol=0.0001, total=  26.3s\n",
      "[CV] C=6, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=6, tol=1e-06, total=  46.5s\n",
      "[CV] C=6, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=6, tol=1e-06, total=  49.4s\n",
      "[CV] C=6, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=6, tol=1e-06, total=  43.9s\n",
      "[CV] C=8, tol=0.01 ...................................................\n",
      "[CV] .................................... C=8, tol=0.01, total=  16.9s\n",
      "[CV] C=8, tol=0.01 ...................................................\n",
      "[CV] .................................... C=8, tol=0.01, total=  16.5s\n",
      "[CV] C=8, tol=0.01 ...................................................\n",
      "[CV] .................................... C=8, tol=0.01, total=  16.6s\n",
      "[CV] C=8, tol=0.0001 .................................................\n",
      "[CV] .................................. C=8, tol=0.0001, total=  24.8s\n",
      "[CV] C=8, tol=0.0001 .................................................\n",
      "[CV] .................................. C=8, tol=0.0001, total=  25.5s\n",
      "[CV] C=8, tol=0.0001 .................................................\n",
      "[CV] .................................. C=8, tol=0.0001, total=  32.0s\n",
      "[CV] C=8, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=8, tol=1e-06, total=  51.4s\n",
      "[CV] C=8, tol=1e-06 ..................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AbhijayGupta/anaconda3/envs/myenv/lib/python3.5/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................... C=8, tol=1e-06, total=  51.4s\n",
      "[CV] C=8, tol=1e-06 ..................................................\n",
      "[CV] ................................... C=8, tol=1e-06, total=  49.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 20.7min finished\n"
     ]
    }
   ],
   "source": [
    "imdb_linearsvm_clf = imdb_linearsvm_clf.fit(train_trigram, IMDB_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_linearsvm_best_params = imdb_linearsvm_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2, 'tol': 0.01}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_linearsvm_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_linearsvm_clf = LinearSVC(tol=imdb_linearsvm_best_params['tol'],\n",
    "                                    C=imdb_linearsvm_best_params['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_linearsvm_clf = imdb_linearsvm_clf.fit(train_trigram, IMDB_train_y)\n",
    "\n",
    "imdb_linearsvm_train_pred = imdb_linearsvm_clf.predict(train_trigram)\n",
    "imdb_linearsvm_valid_pred = imdb_linearsvm_clf.predict(valid_trigram)\n",
    "imdb_linearsvm_test_pred = imdb_linearsvm_clf.predict(test_trigram)\n",
    "\n",
    "imdb_linearsvm_train_f1 = f1_score(IMDB_train_y, imdb_linearsvm_train_pred, average='micro')\n",
    "imdb_linearsvm_valid_f1 = f1_score(IMDB_valid_y, imdb_linearsvm_valid_pred, average='micro')\n",
    "imdb_linearsvm_test_f1 = f1_score(IMDB_test_y, imdb_linearsvm_test_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Linear SVM Train F1 Score: 1.0\n",
      "IMDB Linear SVM Valid F1 Score: 0.5479\n",
      "IMDB Linear SVM Test F1 Score: 0.50316\n",
      "IMDB Best Linear SVM Parameters:  {'C': 2, 'tol': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print(\"IMDB Linear SVM Train F1 Score:\", imdb_linearsvm_train_f1)\n",
    "print(\"IMDB Linear SVM Valid F1 Score:\", imdb_linearsvm_valid_f1)\n",
    "print(\"IMDB Linear SVM Test F1 Score:\", imdb_linearsvm_test_f1)\n",
    "print(\"IMDB Best Linear SVM Parameters: \", imdb_linearsvm_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
