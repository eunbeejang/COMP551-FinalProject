{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import h5py\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "IMDB_train = pd.read_csv('./IMDB-train.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_train_y = IMDB_train[:][1]\n",
    "IMDB_valid = pd.read_csv('./IMDB-valid.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_valid_y = IMDB_valid[:][1]\n",
    "IMDB_test = pd.read_csv('./IMDB-test.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_test_y = IMDB_test[:][1]\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [IMDB_train, IMDB_valid]\n",
    "frames_y = [IMDB_train_y, IMDB_valid_y]\n",
    "IMDB_train = pd.concat(frames)\n",
    "IMDB_train_y = pd.concat(frames_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    new_data = []\n",
    "    #i = 0\n",
    "    for sentence in (data[:][0]):\n",
    "        #clean = re.compile('<.*?>')\n",
    "        new_sentence = re.sub('<.*?>', '', sentence) # remove HTML tags\n",
    "        new_sentence = re.sub(r'[^\\w\\s]', '', new_sentence) # remove punctuation\n",
    "        new_sentence = new_sentence.lower() # convert to lower case\n",
    "        if new_sentence != '':\n",
    "            new_data.append(new_sentence)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef rm_stopwords(data):\\n    new_data = []\\n    for sent in data:\\n        new_data.append([w for w in sent if w not in stopwords])\\n    return new_data\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def rm_stopwords(data):\n",
    "    new_data = []\n",
    "    for sent in data:\n",
    "        new_data.append([w for w in sent if w not in stopwords])\n",
    "    return new_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<<<<IGNORE THESE FOR NOW>>>>\\n\\ndef tokenize(data):\\n    new_data = []\\n    for sentence in (data):\\n        new_sentence = nltk.word_tokenize(sentence)\\n        new_data.append(new_sentence)\\n    return new_data        \\n\\ndef stem_lem(data):\\n    new_data = []\\n    for sent in data:\\n        this_sent = []\\n        for w in test3:\\n            w = stemmer.stem(w)\\n            w = lemmatizer.lemmatize(w)\\n            this_sent.append(stemmer.stem(w))\\n        new_data.append(this_sent)\\n    return new_data\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "<<<<IGNORE THESE FOR NOW>>>>\n",
    "\n",
    "def tokenize(data):\n",
    "    new_data = []\n",
    "    for sentence in (data):\n",
    "        new_sentence = nltk.word_tokenize(sentence)\n",
    "        new_data.append(new_sentence)\n",
    "    return new_data        \n",
    "\n",
    "def stem_lem(data):\n",
    "    new_data = []\n",
    "    for sent in data:\n",
    "        this_sent = []\n",
    "        for w in test3:\n",
    "            w = stemmer.stem(w)\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            this_sent.append(stemmer.stem(w))\n",
    "        new_data.append(this_sent)\n",
    "    return new_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMDB_train = preprocessing(IMDB_train)\n",
    "#IMDB_valid = preprocessing(IMDB_valid)\n",
    "IMDB_test = preprocessing(IMDB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train_tok = tokenize(IMDB_train)\n",
    "#IMDB_valid_tok = tokenize(IMDB_valid)\n",
    "#IMDB_test_tok = tokenize(IMDB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train_sl = stem_lem(IMDB_train_tok)\n",
    "#IMDB_valid_sl = stem_lem(IMDB_valid_tok)\n",
    "#IMDB_test_sl = stem_lem(IMDB_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMDB_train_stop = rm_stopwords(IMDB_train_sl)\n",
    "#IMDB_valid_stop = rm_stopwords(IMDB_valid_sl)\n",
    "#IMDB_test_stop = rm_stopwords(IMDB_test_sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of n-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "unigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 1), stop_words='english', max_features =30000)\n",
    "bigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(2, 2), stop_words='english', max_features =30000)\n",
    "trigram = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(3, 3), stop_words='english', max_features =30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigram = unigram.fit_transform(IMDB_train).toarray()\n",
    "test_unigram = unigram.transform(IMDB_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bigram = bigram.fit_transform(IMDB_train).toarray()\n",
    "test_bigram = bigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_trigram = trigram.fit_transform(IMDB_train).toarray()\n",
    "test_trigram = trigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "unigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 1), stop_words=None, max_features =30000)\n",
    "bigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(2, 2), stop_words=None, max_features =30000)\n",
    "trigram_w_sw = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(3, 3), stop_words=None, max_features =30000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unigram_w_sw = unigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_unigram_w_sw = unigram_w_sw.transform(IMDB_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bigram_w_sw = bigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_bigram_w_sw = bigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_trigram_w_sw = trigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_trigram_w_sw = trigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Naive_Bayes_B(train_data, train_label, valid_data, valid_label, test_data, test_label):\n",
    "    tuned_parameters = [{'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]}]\n",
    "    test_valid_fold = np.r_[ np.full(train_label.shape[0], -1),np.ones(valid_label.shape[0])]\n",
    "    ps = PredefinedSplit(test_valid_fold)\n",
    "\n",
    "    clf = BernoulliNB()\n",
    "    #clf.fit(np.r_[train_data,valid_data], np.r_[train_label,valid_label])\n",
    "    clf = GridSearchCV(clf, tuned_parameters, refit=True, scoring='accuracy', cv=ps, return_train_score=True)\n",
    "    clf.fit(np.r_[train_data,valid_data], np.r_[train_label,valid_label])\n",
    "    #y_pred = clf.predict(yelp_train_x)\n",
    "\n",
    "    train_scores = clf.cv_results_['mean_train_score']\n",
    "    print('train_scores:',train_scores)\n",
    "    test_scores = clf.cv_results_['mean_test_score']\n",
    "    print('valid_scores:',test_scores)\n",
    "    params = clf.cv_results_['params']\n",
    "    print('params:', params)\n",
    "    best_param = clf.best_params_ \n",
    "    print('best_param', best_param)\n",
    "    best_estimator = clf.best_estimator_  \n",
    "    print('best_estimator', best_estimator)\n",
    "    best_score = clf.best_score_\n",
    "    print('best_score', best_score)\n",
    "\n",
    "    clf = BernoulliNB(alpha = best_param['alpha'])\n",
    "    clf.fit(train_data, train_label)\n",
    "    \n",
    "    y_pred_train = clf.predict(train_data)    \n",
    "    y_pred_valid = clf.predict(valid_data)\n",
    "    y_pred_test = clf.predict(test_data)\n",
    "    f1_train= f1_score(train_label, y_pred_train, average='micro')\n",
    "    f1_valid= f1_score(valid_label, y_pred_valid, average='micro')\n",
    "    f1_test= f1_score(test_label, y_pred_test, average='micro')\n",
    "    print('f1 (train): ', f1_train)\n",
    "    print('f1 (valid): ', f1_valid)\n",
    "    print('f1 (test): ', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.92406667 0.92293333 0.92053333 0.91553333 0.90793333 0.89      ]\n",
      "valid_scores: [0.8052 0.8135 0.8261 0.8345 0.8395 0.8393]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 0.1}\n",
      "best_estimator BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.8395\n",
      "f1 (train):  0.9079333333333335\n",
      "f1 (valid):  0.8395000000000001\n",
      "f1 (test):  0.80388\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes_B(train_unigram[:15000],IMDB_train_y[:15000],train_unigram[15000:],IMDB_train_y[15000:],test_unigram,IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores: [0.92706667 0.92586667 0.92333333 0.91866667 0.91033333 0.8942    ]\n",
      "valid_scores: [0.8145 0.8247 0.8335 0.8404 0.8468 0.8453]\n",
      "params: [{'alpha': 1e-05}, {'alpha': 0.0001}, {'alpha': 0.001}, {'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
      "best_param {'alpha': 0.1}\n",
      "best_estimator BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "best_score 0.8468\n",
      "f1 (train):  0.9103333333333333\n",
      "f1 (valid):  0.8468\n",
      "f1 (test):  0.81612\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes_B(train_unigram_w_sw[:15000],IMDB_train_y[:15000],train_unigram_w_sw[15000:],IMDB_train_y[15000:],test_unigram_w_sw,IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Naive_Bayes(train_data, train_label, test_data, test_label, cv):\n",
    "\n",
    "    tuned_parameters = [{'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]}]\n",
    "    clf = BernoulliNB()\n",
    "    clf = GridSearchCV(clf, tuned_parameters, refit=True, scoring='accuracy', cv=cv, return_train_score=True)\n",
    "    clf.fit(train_data, train_label)\n",
    "\n",
    "    \n",
    "    train_scores = clf.cv_results_['mean_train_score']\n",
    "    print('train_scores:',train_scores)\n",
    "    test_scores = clf.cv_results_['mean_test_score']\n",
    "    print('valid_scores:',test_scores)\n",
    "    params = clf.cv_results_['params']\n",
    "    print('params:', params)\n",
    "    best_param = clf.best_params_ \n",
    "    print('best_param', best_param)\n",
    "    best_estimator = clf.best_estimator_  \n",
    "    print('best_estimator', best_estimator)\n",
    "    best_score = clf.best_score_\n",
    "    print('best_score', best_score)\n",
    "\n",
    "    clf = BernoulliNB(alpha = best_param['alpha'])\n",
    "    clf.fit(train_data, train_label)\n",
    "    \n",
    "    y_pred_train = clf.predict(train_data)    \n",
    "    y_pred_test = clf.predict(test_data)\n",
    "    f1_train= f1_score(train_label, y_pred_train, average='micro')\n",
    "    f1_test= f1_score(test_label, y_pred_test, average='micro')\n",
    "    print('f1 (train): ', f1_train)\n",
    "    print('f1 (test): ', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive_Bayes(train_unigram,np.asarray(IMDB_train_y),test_unigram,np.asarray(IMDB_test_y), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Naive_Bayes(train_unigram_w_sw,IMDB_train_y,test_unigram_w_sw,IMDB_test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
