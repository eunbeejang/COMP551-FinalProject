{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "IMDB_train = pd.read_csv('./Dataset/Input/IMDB-train.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_train_y = IMDB_train[:][1]\n",
    "IMDB_valid = pd.read_csv('./Dataset/Input/IMDB-valid.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_valid_y = IMDB_valid[:][1]\n",
    "IMDB_test = pd.read_csv('./Dataset/Input/IMDB-test.txt', sep='\\t', encoding='latin-1', header=None)\n",
    "IMDB_test_y = IMDB_test[:][1]\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [IMDB_train, IMDB_valid]\n",
    "frames_y = [IMDB_train_y, IMDB_valid_y]\n",
    "IMDB_train = pd.concat(frames)\n",
    "IMDB_train_y = pd.concat(frames_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    new_data = []\n",
    "    #i = 0\n",
    "    for sentence in (data[:][0]):\n",
    "        #clean = re.compile('<.*?>')\n",
    "        new_sentence = re.sub('<.*?>', '', sentence) # remove HTML tags\n",
    "        new_sentence = re.sub(r'[^\\w\\s]', '', new_sentence) # remove punctuation\n",
    "        new_sentence = new_sentence.lower() # convert to lower case\n",
    "        if new_sentence != '':\n",
    "            new_data.append(new_sentence)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_train = preprocessing(IMDB_train)\n",
    "IMDB_test = preprocessing(IMDB_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of n-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_sents(data, target):\n",
    "    new_data = []\n",
    "    new_target = []\n",
    "    for i in range(0,len(data)):\n",
    "        if len(list(set(data[i]))) != 1:\n",
    "            new_data.append(data[i])\n",
    "            new_target.append(target[i])\n",
    "    return new_data, new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = TfidfVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 1), stop_words='english', \n",
    "                          max_features =30000)\n",
    "bigram = TfidfVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 2), stop_words='english', \n",
    "                         max_features =30000)\n",
    "trigram = TfidfVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 3), stop_words='english', \n",
    "                          max_features =30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigram = unigram.fit_transform(IMDB_train).toarray()\n",
    "test_unigram = unigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigram = bigram.fit_transform(IMDB_train).toarray()\n",
    "test_bigram = bigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trigram = trigram.fit_transform(IMDB_train).toarray()\n",
    "test_trigram = trigram.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_w_sw = TfidfVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 1), stop_words=None, \n",
    "                               max_features =30000, binary=True)\n",
    "bigram_w_sw = TfidfVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 2), stop_words=None, \n",
    "                              max_features =30000, binary=True)\n",
    "trigram_w_sw = TfidfVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', ngram_range=(1, 3), stop_words=None, \n",
    "                               max_features =30000, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigram_w_sw = unigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_unigram_w_sw = unigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigram_w_sw = bigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_bigram_w_sw = bigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trigram_w_sw = trigram_w_sw.fit_transform(IMDB_train).toarray()\n",
    "test_trigram_w_sw = trigram_w_sw.transform(IMDB_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linearsvm(imdb_linearsvm_clf, train_input, train_output, test_input, test_output):\n",
    "    imdb_linearsvm_clf = imdb_linearsvm_clf.fit(train_input, train_output)\n",
    "    imdb_linearsvm_best_params = imdb_linearsvm_clf.best_params_\n",
    "    print(imdb_linearsvm_best_params)\n",
    "    # make classifier with best parameters found\n",
    "    imdb_linearsvm_clf = LinearSVC(tol=imdb_linearsvm_best_params['tol'],\n",
    "                                    C=imdb_linearsvm_best_params['C'])\n",
    "    \n",
    "    imdb_linearsvm_clf = imdb_linearsvm_clf.fit(train_input, train_output)\n",
    "    \n",
    "    # make predictions\n",
    "    imdb_linearsvm_train_pred = imdb_linearsvm_clf.predict(train_input)\n",
    "    imdb_linearsvm_test_pred = imdb_linearsvm_clf.predict(test_input)\n",
    "\n",
    "    # calculate accuracy\n",
    "    imdb_linearsvm_train_accuracy = accuracy_score(train_output, imdb_linearsvm_train_pred)\n",
    "    imdb_linearsvm_test_accuracy = accuracy_score(test_output, imdb_linearsvm_test_pred)\n",
    "    \n",
    "    return [imdb_linearsvm_train_accuracy, imdb_linearsvm_test_accuracy, imdb_linearsvm_best_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_svm_clf():\n",
    "    imdb_linearsvm_clf = LinearSVC(max_iter=35000)\n",
    "    list_C = np.random.uniform(low=1, high=20, size=20)\n",
    "    list_tol = np.random.uniform(low=10**(-6), high=10**(-1), size=20)\n",
    "    # parameter grid to check against\n",
    "    # for hyperparameter tuning\n",
    "    tuned_parameters = {'C': list_C, 'tol': list_tol}\n",
    "    imdb_linearsvm_clf = RandomizedSearchCV(imdb_linearsvm_clf, tuned_parameters, scoring='accuracy', cv=3, verbose=2, \n",
    "                                            n_iter=20)\n",
    "    return imdb_linearsvm_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without stop words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_results = run_linearsvm(init_svm_clf(), train_unigram, IMDB_train_y, test_unigram, IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear SVM Train Unigram Accuracy:\", unigram_results[0])\n",
    "# print(\"IMDB Linear SVM Valid F1 Score:\", imdb_linearsvm_valid_f1)\n",
    "print(\"Linear SVM Test Unigram Accuracy:\", unigram_results[1])\n",
    "print(\"Best Linear SVM (Unigram) Parameters:\", unigram_results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_results = run_linearsvm(init_svm_clf(), train_bigram, IMDB_train_y, test_bigram, IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear SVM Train Bigram Accuracy:\", bigram_results[0])\n",
    "# print(\"IMDB Linear SVM Valid F1 Score:\", imdb_linearsvm_valid_f1)\n",
    "print(\"Linear SVM Test Bigram Accuracy:\", bigram_results[1])\n",
    "print(\"Best Linear SVM (Bigram) Parameters:\", bigram_results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_results = run_linearsvm(init_svm_clf(), train_trigram, IMDB_train_y, test_trigram, IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear SVM Train Trigram Accuracy:\", trigram_results[0])\n",
    "# print(\"IMDB Linear SVM Valid F1 Score:\", imdb_linearsvm_valid_f1)\n",
    "print(\"Linear SVM Test Trigram Accuracy:\", trigram_results[1])\n",
    "print(\"Best Linear SVM (Trigram) Parameters:\", trigram_results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_w_sw_results = run_linearsvm(init_svm_clf(), train_unigram_w_sw, IMDB_train_y, \n",
    "                                     test_unigram_w_sw, IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear SVM Train Unigram w/ SW Accuracy:\", unigram_w_sw_results[0])\n",
    "# print(\"IMDB Linear SVM Valid F1 Score:\", imdb_linearsvm_valid_f1)\n",
    "print(\"Linear SVM Test Unigram w/ SW Accuracy:\", unigram_w_sw_results[1])\n",
    "print(\"Best Linear SVM (Unigram) w/ SW Parameters:\", unigram_w_sw_results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_w_sw_results = run_linearsvm(init_svm_clf(), train_bigram_w_sw, IMDB_train_y, \n",
    "                                     test_bigram_w_sw, IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear SVM Train Bigram w/ SW Accuracy:\", bigram_w_sw_results[0])\n",
    "# print(\"IMDB Linear SVM Valid F1 Score:\", imdb_linearsvm_valid_f1)\n",
    "print(\"Linear SVM Test Bigram w/ SW Accuracy:\", bigram_w_sw_results[1])\n",
    "print(\"Best Linear SVM (Bigram) w/ SW Parameters:\", bigram_w_sw_results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_w_sw_results = run_linearsvm(init_svm_clf(), train_trigram_w_sw, IMDB_train_y, \n",
    "                                     test_trigram_w_sw, IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear SVM Train Trigram w/ SW Accuracy:\", trigram_w_sw_results[0])\n",
    "# print(\"IMDB Linear SVM Valid F1 Score:\", imdb_linearsvm_valid_f1)\n",
    "print(\"Linear SVM Test Trigram w/ SW Accuracy:\", trigram_w_sw_results[1])\n",
    "print(\"Best Linear SVM (Trigram) w/ SW Parameters:\", trigram_w_sw_results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
